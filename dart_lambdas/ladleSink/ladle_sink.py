import json
import os
from json import JSONDecodeError

from dart_lambdas.common.custom_logging import LOG, log_environment
from dart_lambdas.ladleSink.workers import get_created_object, move_processed_object, get_key_from_ladle_doc_id, \
    get_created_object_metadata, post_retrieved_object_and_metadata


def lambda_handler(event, context):
    # Debugging
    log_environment(event)

    success, message = handle_records(event['Records'])

    if success:
        return {
            'statusCode': 200,
            'body': json.dumps(message)
        }
    else:
        return {
            'statusCode': 500,
            'body': json.dumps(message)
        }


def handle_records(records_list):
    # Fetch relevant environment variables
    S3_BUCKET_IN = os.environ.get('S3_BUCKET_IN')
    S3_BUCKET_PROCESSED = os.environ.get('S3_BUCKET_PROCESSED')
    DART_URL = os.environ.get('DART_URL')
    SUBMISSION_PORT = os.environ.get('SUBMISSION_PORT')
    SUBMISSION_ENDPOINT = os.environ.get('SUBMISSION_ENDPOINT')

    for record in records_list:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        if key.endswith( ".meta" ):
            return succeed( f"metadata file ({key}): no need to pass to ladle" )
        if not key.endswith( ".raw"):
            LOG( f"not a raw document: {key} does not end with '.raw' extension" )
            return succeed( f"not a raw document: {key} does not end with '.raw' extension" )
        if len(key.split(".")) < 3:
            LOG( f"raw document is missing extension: {key}" )
            return fail( f"raw document is missing extension: {key}" )

        # Make sure request is coming from correct bucket (should be impossible not to)
        if bucket != S3_BUCKET_IN:
            return fail(f'Create-Object event did not come from {S3_BUCKET_IN}')

        LOG(f'GETTING METADATA OBJECT FROM {S3_BUCKET_IN}')

        # Get metadata for object referenced in event record
        get_success_metadata, key_metadata, metadata, get_err_metadata = get_created_object_metadata(record, bucket)

        if not get_success_metadata:
            return fail(f'Unable to retrieve {key_metadata} from {bucket}: {get_err_metadata}')

        # Get object referenced in event record
        get_success, key, raw_doc, get_err = get_created_object(record, bucket)

        if not get_success:
            return fail(f'Unable to retrieve {key} from {bucket}: {get_err}')

        post_key = ".".join(key.split(".")[0:-1])

        LOG(f'POSTING FILE TO LADLE: {post_key} ---> {DART_URL}:{SUBMISSION_PORT}{SUBMISSION_ENDPOINT}')

        # Post object to Ladle using correct key as filename
        post_success, post_response = post_retrieved_object_and_metadata(post_key, raw_doc, metadata)

        if not post_success:
            return fail(f"Unable to submit {key} to ladle as {post_key}: {post_response}")

        # Change the filename to the document id generated by Ladle
        final_key = get_key_from_ladle_doc_id(post_response, key)
        final_key_metadata = final_key.split('.')[0] + '.meta'

        if S3_BUCKET_PROCESSED is None or S3_BUCKET_PROCESSED == "":
            return succeed(f'Successfully posted {key} to {DART_URL} as {post_key}. No secondary bucket set for processed documents')

        LOG(f'PUTTING METADATA IN SECOND S3_BUCKET: {key_metadata} ---> {S3_BUCKET_PROCESSED} as {final_key_metadata}')

        move_success_metadata, move_err_metadata = move_processed_object(key_metadata, S3_BUCKET_IN, final_key_metadata, S3_BUCKET_PROCESSED)

        if not move_success_metadata:
            return fail(f'Unable to move {key_metadata} from {S3_BUCKET_IN} to {final_key_metadata} in {S3_BUCKET_PROCESSED}: {move_err_metadata}')

        LOG(f'PUTTING FILE IN SECOND S3_BUCKET: {key} ---> {S3_BUCKET_PROCESSED} as {final_key}')

        # Move object from ingest bucket to processed bucket
        move_success, move_err = move_processed_object(key, S3_BUCKET_IN, final_key, S3_BUCKET_PROCESSED)

        if not move_success:
            return_meta_success, return_meta_err = move_processed_object(final_key_metadata, S3_BUCKET_PROCESSED, key_metadata, S3_BUCKET_IN)
            if not return_meta_success:
                return fail(f'Unable to move {key} from {S3_BUCKET_IN} to {final_key} in {S3_BUCKET_PROCESSED}: {move_err}\nand unable to return metadata ({final_key_metadata}) from ${S3_BUCKET_PROCESSED}')
            else:
                return fail(f'Unable to move {key} from {S3_BUCKET_IN} to {final_key} in {S3_BUCKET_PROCESSED}: {move_err}')

        return succeed(f'Successfully pulled {key} from {S3_BUCKET_IN}, posted it to {DART_URL} as {post_key}, and moved it to {S3_BUCKET_PROCESSED} as {final_key}')


def fail(msg):
    print(msg)
    return False, msg


def succeed(msg):
    LOG(msg)
    return True, msg
